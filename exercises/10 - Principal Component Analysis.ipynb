{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62c1dc5-a47b-400d-84c3-e68cb24d8498",
   "metadata": {},
   "source": [
    "# Matrix Analysis 2022 - EE312\n",
    "## Week 10 - Principal Component Analysis (PCA)\n",
    "[LTS2](https://lts2.epfl.ch)\n",
    "\n",
    "PCA is a classic technique for dimensionality reduction. As you will see in this notebook, this method uses  projections and eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9eaa72-f62a-4417-a5e9-0f47f1651d7b",
   "metadata": {},
   "source": [
    "## 1. Eigenvalues and PCA\n",
    "\n",
    "Let us consider $N$ data points $\\{x_k\\}, k=1, ..., N$ in $\\mathbb{R}^d$. During the rest of the exercise we will make the assumption that the mean value of these data points is 0, i.e. $\\frac{1}{N}\\sum_{k=1}^Nx_k=0$. We will denote by $X$ the $N\\times d$ matrix s.t. :\n",
    "\n",
    "$\n",
    "X = \\begin{pmatrix}\n",
    "x_1^T\\\\\n",
    "x_2^T\\\\ \\vdots \\\\ x_N^T \\end{pmatrix}$\n",
    "\n",
    "**1.1**\n",
    "Write the projection of the data points $x_k$ on a unit-norm vector $u\\in\\mathbb{R}^d$ using a matrix operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bfd7ec-efc5-4ebf-a551-2f5454af5277",
   "metadata": {},
   "source": [
    "**Answer** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855cf9de-e767-482f-9a52-31e6b0e08659",
   "metadata": {},
   "source": [
    "**1.2** Compute the variance of the projections of $x_k$ on $u$. **Reminder:** the variance of $\\{b_k\\}, k=1, ..., N$ is $\\frac{1}{N}\\sum_{k=1}^N||b_k - \\bar{b}||^2$, where $\\bar{b}$ is the mean value of the $b_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56d9e2-367e-44e4-b19d-962687219ff4",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb2244-00c4-435e-ac7e-13efc48743ba",
   "metadata": {},
   "source": [
    "**1.3** Let us define the matrix $C = \\frac{1}{N}X^TX$ (referred to as the **sample covariance matrix** in the litterature). What are the properties of this matrix ? What is the implication on its eigenvalues ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0427c7-0c85-4e42-8a3f-f3f273086886",
   "metadata": {},
   "source": [
    "**Answer** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8368fa14-0308-46e7-ba2a-f8d21d56bcf1",
   "metadata": {},
   "source": [
    "**1.4** PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. The following picture illustrate the principle for (blue) points in $\\mathbb{R}^2$. Intuitively, the variance of the projected points (in red) will be maximized when the direction of projection matches the main direction of the point cloud in the picture.\n",
    "\n",
    "![PCA](images/pca.gif)\n",
    "\n",
    "We will now try to find a vector $u$, $||u||=1$, s.t. the variance of the projection of the data on this vector is maximal. Let us order the eigenvalues of $C$ in a decreasing order, i.e. $\\lambda_1\\geq \\lambda_2\\geq...\\geq \\lambda_d$. Show that the eigenvector associated with the largest eigenvalue maximizes the variance of the projection of $x_k$. This will be the first vector used for the first principal component. (Hint: consider the orthonormal basis formed by the eigenvectors of $C$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c845f5-c563-46c8-8b95-a45fd5e686cb",
   "metadata": {},
   "source": [
    "**Answer** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d5f24-b360-4db5-999f-c0b924a75199",
   "metadata": {},
   "source": [
    "**1.5** What is the second vector that will maximize the variance of the $x_k$ minus their projection on the first principal component vector (i.e. eigenvector associated to $\\lambda_1$) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c4289-265c-48c2-9038-5f9e3568a6a4",
   "metadata": {},
   "source": [
    "**Answer** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a9f74-c97c-44b6-bcb0-c341c8aabd36",
   "metadata": {},
   "source": [
    "## 2. Applying PCA to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c6e04-ecb4-49b6-ae26-a29a2b19290f",
   "metadata": {},
   "source": [
    "After completing the previous part, you probably figured that PCA is achieved by iterating projections of residuals on eigenvectors of the covariance matrix. In this exercise, you will apply the PCA to a specific dataset.\n",
    "\n",
    "**Warning** Computing the eigenvectors and eigenvalues can take time (> 1 minute) and is too slow on noto. Use either a local installation or [google colab](https://colab.research.google.com/)\n",
    "\n",
    "### 2.1 Loading the dataset\n",
    "The [\"Olivetti faces dataset\"](https://scikit-learn.org/stable/datasets/real_world.html?highlight=olivetti) is made of 400 64x64 images (represented as vectors containing 4096 elements). The dataset is made of pictures of 40 persons, with varying light conditions, facial expressions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaaffb2-9bd3-44f7-87ef-1a8e636889a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea92d6-5f26-4ae3-9f42-d5c1de786603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "faces, labels = fetch_olivetti_faces(return_X_y=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b71c6-cc58-4639-8ffc-9f7b78420427",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(faces[2,:], (64,64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d74c26-1b9e-4c55-8586-5f68334449d8",
   "metadata": {},
   "source": [
    "The first step to perform the PCA is to remove the mean value from the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c293fe6-54d4-409e-8cf4-2b145b41cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_face = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fec8d0-2f2a-4a66-8722-5c0138e712fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(mean_face, (64,64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80647d21-857c-42c5-a228-6955de2e3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_zero_centered = faces - mean_face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d188a-170e-4755-a3fc-c2c3ef1342d1",
   "metadata": {},
   "source": [
    "### 2.2 Covariance matrix\n",
    "As seen in the theoretical part, you need compute the covariance matrix $C$ and compute its eigenvalues and eigenvectors (use the [numpy.linalg.eigh](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html) function).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfdcff-bba0-4f00-a719-635917f3f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb9962-d3d8-4470-af86-77d3c9fa7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15891ca-27b6-4ffe-b2a6-6b8c6e859252",
   "metadata": {},
   "source": [
    "Using the [argsort](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) function, sort the eigenvalues and eigenvectors appropriately (do not trust Numpy's `eigh` documentation !). Be careful, `argsort` sorts in ascending order only, do not forget to reverse the array !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d6b6f-7a40-425e-b1ab-8269347a40db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b69e7-a150-473e-b463-2e767075c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_eig_vals = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edc15a-e2d4-402e-8be7-75287234f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_eig_vecs = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f301e2d-53e9-483e-81e8-08c3abfe5eac",
   "metadata": {},
   "source": [
    "Display the eigenvectors associated with the largest eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b766525-826d-48ab-9600-124205e8cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a127c1-c26c-424d-a6b6-3bf024494653",
   "metadata": {},
   "source": [
    "### 2.3 PCA \n",
    "We are finally ready to write a function that performs a PCA, i.e. given input data, returns its projection on the $n$ largest eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc7b51-f001-4ddb-ba90-8a4e4d4791dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(input_data, eig_vectors, n):\n",
    "    # your code here\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759cc8aa-9d6e-4e95-83ab-f87dca11eaa8",
   "metadata": {},
   "source": [
    "For a given input image, compute an approximation using $n$ principal components. How many components (approximately) do you need to have a result that is close to the original image ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d31a5b-2ba0-4b1d-8169-7b2179ad59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_approx(pca_projection, eig_vectors, n):\n",
    "    # your code here\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e546ea2-d1f6-4b5c-9caf-e54f9822cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 # update this number\n",
    "img = faces_zero_centered[40, :] # try other images too\n",
    "cc = pca(img, sorted_eig_vecs, n)\n",
    "approx = pca_approx(cc, sorted_eig_vecs, n)\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.reshape(img, (64, 64)))\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.reshape(approx, (64, 64)))\n",
    "cc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5ffb8-d775-4779-8901-4a5b4017049a",
   "metadata": {},
   "source": [
    "Depending on the image, 100 to 200 components yield a good approximation of the initial image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee914f-3d69-43e3-95c8-b7d3bdd39578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
